在atom的主板上，连接本地的inter网卡与远端主板的inter网卡，打出的性能基本一致
e1000e使用的MSI-X（使用lspci -v查看）
使用了MSI-X就会使用cpu亲和（cat /proc/interrupts 和 cat /proc/irq/x/smp_affinity 查看）
情况1：
测试64bit小包转发，本地测试是，eth2使用cpu0，eth3使用cpu1。远端测试，eth2与eth3都在使用cpu0，因此发现发包+收包总的中断数小于本地测试时的中断数。
情况2：
查看网卡中断的cpu亲和的时候，发现在未转发和转发的情况下，cpu亲和是会改变的，可能是/etc/init.d/irqblance在起作用。
情况3：
收包所消耗的cpu资源远不是发包可以比，在无法达到线速的时候，收包的cpu的sirq会达到100%，但发包可以保持在15%以下，甚至更低。在1024字节达到线速之后，收包cpu的sirq为20%左右，发包10%以下。
情况4：
没有达到线速，atom板子的性能是每个中断8个包左右，达到线速5个包（因为在1024字节下，每秒2w中断左右，每个中断也只有5个包）。
情况5：
通过降低速率，在64字节的打500M、250M、50M的流量，中断数并没有明显的减少，都保持在17000中断数以上，在500M和250M由于没有线速，还是每个中断大概8~9个包，但50M时，已经达到线速，每个中断大概1.5个包。

atom的瓶颈，主要是在收包，会占用大量的cpu时间在sirq中。网卡正常流量的数据下，中断数一般在固定范围内的，也就是说每秒的时间都被这些中断给分割了差不多等分时间t。
当报文长度变大时，t能处理的报文数目越少。当大包报文数目小于t时间内cpu的处理能力时，可以达到线速。小包报文数目太多，大于t时间内cpu处理能力则会丢包（64字节可以说是远远不足，应该每个中断达到70个报文才能线速，实际上只有8个报文）。
cpu的性能和t时间内的处理报文个数，则决定了网卡在系统中的性能。
1、提高cpu性能，使用频率更高的cpu，显然可以提高速率。
2、使用netmap这种架构，简化了报文转发流程，减少中断数目，增加了一个中断处理报文的能力，使得atom也能在64字节的报下达到转发线速（netmap每个中断可以处理256、512个报文，根据驱动定义）。

